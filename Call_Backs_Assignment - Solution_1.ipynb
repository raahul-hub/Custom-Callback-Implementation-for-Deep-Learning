{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQDRNrY2NCXf"
   },
   "source": [
    "<pre>\n",
    "1. Download the data.\n",
    "\n",
    "2. Code the model to classify data like below image\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n",
    "3. Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.\n",
    "\n",
    "4. Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "5. you have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "6. If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "7. You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "8. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "9. use cross entropy as loss function\n",
    "\n",
    "10. Try the architecture params as given below. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense,Input,Activation, BatchNormalization, Dropout, Flatten, MaxPooling1D\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('/content/drive/MyDrive/20 Working with Callbacks/Callbacks/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "data_features = data.copy()\n",
    "data_labels = data_features.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450564</td>\n",
       "      <td>1.074305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.967682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117326</td>\n",
       "      <td>0.971521</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982179</td>\n",
       "      <td>-0.380408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.720352</td>\n",
       "      <td>0.955850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>-0.491252</td>\n",
       "      <td>-0.561558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>-0.813124</td>\n",
       "      <td>0.049423</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>-0.010594</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.671827</td>\n",
       "      <td>0.804306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>-0.854865</td>\n",
       "      <td>-0.588826</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1        f2  label\n",
       "0      0.450564  1.074305    0.0\n",
       "1      0.085632  0.967682    0.0\n",
       "2      0.117326  0.971521    1.0\n",
       "3      0.982179 -0.380408    0.0\n",
       "4     -0.720352  0.955850    0.0\n",
       "...         ...       ...    ...\n",
       "19995 -0.491252 -0.561558    0.0\n",
       "19996 -0.813124  0.049423    1.0\n",
       "19997 -0.010594  0.138790    1.0\n",
       "19998  0.671827  0.804306    0.0\n",
       "19999 -0.854865 -0.588826    0.0\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(data_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_features, data_labels, train_size=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(list(data.label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450564</td>\n",
       "      <td>1.074305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.967682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117326</td>\n",
       "      <td>0.971521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982179</td>\n",
       "      <td>-0.380408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.720352</td>\n",
       "      <td>0.955850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>-0.491252</td>\n",
       "      <td>-0.561558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>-0.813124</td>\n",
       "      <td>0.049423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>-0.010594</td>\n",
       "      <td>0.138790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0.671827</td>\n",
       "      <td>0.804306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>-0.854865</td>\n",
       "      <td>-0.588826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f1        f2\n",
       "0      0.450564  1.074305\n",
       "1      0.085632  0.967682\n",
       "2      0.117326  0.971521\n",
       "3      0.982179 -0.380408\n",
       "4     -0.720352  0.955850\n",
       "...         ...       ...\n",
       "19995 -0.491252 -0.561558\n",
       "19996 -0.813124  0.049423\n",
       "19997 -0.010594  0.138790\n",
       "19998  0.671827  0.804306\n",
       "19999 -0.854865 -0.588826\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Loss History**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:Call_Backs_Reference.ipynb from AAIC\n",
    "\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        ## on begin of training, we are creating a instance varible called history\n",
    "        ## it is a dict with keys [loss, acc, val_loss, val_acc]\n",
    "        self.history={'loss': [],'acc': [],'val_loss': [],'val_acc': []}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        ## on end of each epoch, we will get logs and update the self.history dict\n",
    "        self.history['loss'].append(logs.get('loss'))\n",
    "        self.history['acc'].append(logs.get('acc'))\n",
    "        if logs.get('val_loss', -1) != -1:\n",
    "            self.history['val_loss'].append(logs.get('val_loss'))\n",
    "        if logs.get('val_acc', -1) != -1:\n",
    "            self.history['val_acc'].append(logs.get('val_acc'))\n",
    "            \n",
    "history_own=LossHistory()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 03.** Write  callback function, that has to print the micro F1 score and AUC score after each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ROC-AUC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:https://stackoverflow.com/questions/59666138/sklearn-roc-auc-score-with-multi-class-ovr-should-have-none-average-available\n",
    "#Code Reference: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "#Reference:https://learning.oreilly.com/library/view/deep-learning-quick/9781788837996/a22485be-e397-4b46-86b2-29b7878953f5.xhtml#:~:text=Let's%20use%20one%20more%20callback,Keras%20is%20actually%20really%20simple.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "class RocCallback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "     \n",
    "        #y_pred_train = np.argmax(model.predict(X_train),axis=1)\n",
    "        y_pred_train = model.predict(self.x)\n",
    "        roc_train = roc_auc_score(self.y, y_pred_train, average='weighted')\n",
    "\n",
    "        y_pred_val = model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val, average='weighted')\n",
    "        print('\\nROC-AUC Train: %s - ROC-AUC Test: %s' % (str(round(roc_train,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "\n",
    "ROC_AUC = RocCallback(training_data=(X_train, Y_train),\n",
    "                  validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:https://stackoverflow.com/questions/59666138/sklearn-roc-auc-score-with-multi-class-ovr-should-have-none-average-available\n",
    "#Code Reference: https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "#Reference:https://learning.oreilly.com/library/view/deep-learning-quick/9781788837996/a22485be-e397-4b46-86b2-29b7878953f5.xhtml#:~:text=Let's%20use%20one%20more%20callback,Keras%20is%20actually%20really%20simple.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "class F1Score(Callback):\n",
    "    #Using __init__ method to access training and validation data\n",
    "    def __init__(self, Xtrain, y_train_encoded, Xtest, y_test_encoded):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test       \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    " \n",
    "        y_pred_train = model.predict_classes(self.X_train)\n",
    "        #for milticlass classification need set average other than binary\n",
    "        F1Score_train = f1_score(self.Y_train, y_pred_train, average='binary',sample_weight=None, zero_division=0)\n",
    "\n",
    "\n",
    "        y_pred_test = model.predict_classes(self.X_test)\n",
    "        F1Score_test = f1_score(self.Y_test, y_pred_test, average='binary',sample_weight=None, zero_division=0)\n",
    "        \n",
    "        print('\\nTraining F1-Score: %s - Testing F1-Score: %s' % (str(round(F1Score_train,5)),str(round(F1Score_test,5))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "\n",
    "F1Score = F1Score(X_train, Y_train,\n",
    "                  X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 04.** Save your model at every epoch if your validation accuracy is improved from previous epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Model Checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"model_autosaved/model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, \n",
    "                             monitor='val_accuracy',  verbose=1, \n",
    "                             save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 05.** you have to decay learning based on below conditions \n",
    "- Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the learning rate by 10%. \n",
    "- Cond2. For every 3rd epoch, decay your learning rate by 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cond1: **ReduceLROnPlateau**\n",
    "   \n",
    "   \n",
    "   If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the learning rate by 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy', factor=0.9, patience=2, verbose=1,\n",
    "            mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cond2: **LearningRateScheduler**\n",
    "\n",
    "For every 3rd epoch, decay your learning rate by 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/39779710/setting-up-a-learningratescheduler-in-keras\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 3 ==0:\n",
    "        return lr*0.95\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "lr_scheduler = LearningRateScheduler(scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 06.** If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:Call_Backs_Reference.ipynb from AAIC\n",
    "class TerminateNaN(tf.keras.callbacks.Callback):    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "terminate_NaN = TerminateNaN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 07.** You have to stop the training if your validation accuracy is not increased in last 2 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Earlystopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_accuracy = EarlyStopping(monitor='val_accuracy', min_delta=0.35, patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 08.** Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:https://www.tensorflow.org/tensorboard/get_started\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "#rm -rf ./logs/\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/\", histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback_list = [ROC_AUC, F1Score, checkpoint, decay_lr, lr_scheduler, terminate_NaN, earlystop_accuracy, tensorboard_callback ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 11,891\n",
      "Trainable params: 11,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(200, activation='tanh', kernel_initializer= tf.keras.initializers.RandomUniform(0,1), input_shape=(n_features,)))\n",
    "model_1.add(Dense(50, activation='tanh', kernel_initializer= tf.keras.initializers.RandomUniform(0,1)))\n",
    "model_1.add(Dense(20, activation='tanh', kernel_initializer= tf.keras.initializers.RandomUniform(0,1)))\n",
    "model_1.add(Dense(10, activation='tanh', kernel_initializer= tf.keras.initializers.RandomUniform(0,1)))\n",
    "model_1.add(Dense(1))\n",
    "\n",
    "print(model_1.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_1\n",
    "\n",
    "print('Architecture:')\n",
    "plot_model(model, to_file='Model_1.png', show_shapes=False, dpi = 96 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0009500000451225787.\n",
      "Epoch 1/100\n",
      " 1/85 [..............................] - ETA: 0s - loss: 4.9961 - accuracy: 0.4300WARNING:tensorflow:From C:\\Users\\asd\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/85 [..............................] - ETA: 22s - loss: 4.6281 - accuracy: 0.4375WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.5611s). Check your callbacks.\n",
      "63/85 [=====================>........] - ETA: 0s - loss: 4.2459 - accuracy: 0.4954\n",
      "ROC-AUC Train: 0.4995 - ROC-AUC Test: 0.4888                                                                                                    \n",
      "WARNING:tensorflow:From <ipython-input-12-c59bb5ac4fd6>:17: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "\n",
      "Training F1-Score: 0.49862 - Testing F1-Score: 0.49054                                                                                                    \n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.48833, saving model to model_autosaved\\model_1.hdf5\n",
      "85/85 [==============================] - 2s 26ms/step - loss: 4.2502 - accuracy: 0.4944 - val_loss: 4.1154 - val_accuracy: 0.4883\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0009500000160187483.\n",
      "Epoch 2/100\n",
      "69/85 [=======================>......] - ETA: 0s - loss: 4.3415 - accuracy: 0.4966\n",
      "ROC-AUC Train: 0.4986 - ROC-AUC Test: 0.4874                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.00047 - Testing F1-Score: 0.00136                                                                                                    \n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.48833 to 0.51000, saving model to model_autosaved\\model_1.hdf5\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 4.3082 - accuracy: 0.4993 - val_loss: 4.2655 - val_accuracy: 0.5100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0009500000160187483.\n",
      "Epoch 3/100\n",
      "81/85 [===========================>..] - ETA: 0s - loss: 4.3764 - accuracy: 0.4965\n",
      "ROC-AUC Train: 0.4977 - ROC-AUC Test: 0.4859                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.0007 - Testing F1-Score: 0.0                                                                                                    \n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.51000\n",
      "85/85 [==============================] - 2s 19ms/step - loss: 4.3816 - accuracy: 0.4965 - val_loss: 4.2851 - val_accuracy: 0.5093\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0009025000152178108.\n",
      "Epoch 4/100\n",
      "64/85 [=====================>........] - ETA: 0s - loss: 4.4136 - accuracy: 0.4993\n",
      "ROC-AUC Train: 0.4975 - ROC-AUC Test: 0.4866                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.48567 - Testing F1-Score: 0.47761                                                                                                    \n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.51000\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0008122500032186509.\n",
      "85/85 [==============================] - 2s 21ms/step - loss: 4.4075 - accuracy: 0.5019 - val_loss: 4.2861 - val_accuracy: 0.4867\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1547daa1e50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_1\n",
    "\n",
    "optimizer_SGD = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.001, momentum=0.9, nesterov=True, name='SGD')\n",
    "\n",
    "filepath=\"model_autosaved/model_1.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, \n",
    "                             monitor='val_accuracy',  verbose=1, \n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "decay_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy', factor=0.9, patience=2, verbose=1,\n",
    "            mode='auto', min_delta=0.001, cooldown=0, min_lr=0.0001)\n",
    "\n",
    "earlystop_accuracy = EarlyStopping(monitor='val_accuracy', min_delta=0.00001, patience=2, verbose=1)\n",
    "\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/model_1\", histogram_freq=1)\n",
    "\n",
    "callback_list = [ROC_AUC, F1Score, checkpoint, decay_lr, \n",
    "                 lr_scheduler, terminate_NaN, earlystop_accuracy, \n",
    "                 tensorboard_callback ]\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer= optimizer_SGD, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), batch_size=200,  \n",
    "          callbacks= callback_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 15)                315       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 544\n",
      "Trainable params: 544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(20, activation='relu', kernel_initializer= tf.keras.initializers.RandomUniform(0,1), input_shape=(n_features,)))\n",
    "model_2.add(Dense(15, activation='relu', kernel_initializer= tf.keras.initializers.RandomUniform(0,1)))\n",
    "model_2.add(Dense(8, activation='relu', kernel_initializer= tf.keras.initializers.RandomUniform(0,1)))\n",
    "model_2.add(Dense(4, activation='relu', kernel_initializer= tf.keras.initializers.RandomUniform(0,1)))\n",
    "model_2.add(Dense(1))\n",
    "\n",
    "print(model_2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0009500000451225787.\n",
      "Epoch 1/100\n",
      " 2/85 [..............................] - ETA: 43s - loss: 7.1726 - accuracy: 0.5350WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0078s vs `on_train_batch_end` time: 1.0480s). Check your callbacks.\n",
      "83/85 [============================>.] - ETA: 0s - loss: 7.7208 - accuracy: 0.4995\n",
      "ROC-AUC Train: 0.532 - ROC-AUC Test: 0.5387                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.0 - Testing F1-Score: 0.0                                                                                                    \n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50967, saving model to model_autosaved\\model_2.hdf5\n",
      "85/85 [==============================] - 3s 33ms/step - loss: 7.7388 - accuracy: 0.4983 - val_loss: 7.5634 - val_accuracy: 0.5097\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0009500000160187483.\n",
      "Epoch 2/100\n",
      "68/85 [=======================>......] - ETA: 0s - loss: 7.7227 - accuracy: 0.4993\n",
      "ROC-AUC Train: 0.532 - ROC-AUC Test: 0.5387                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.0 - Testing F1-Score: 0.0                                                                                                    \n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.50967\n",
      "85/85 [==============================] - 1s 12ms/step - loss: 7.7388 - accuracy: 0.4983 - val_loss: 7.5634 - val_accuracy: 0.5097\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0009500000160187483.\n",
      "Epoch 3/100\n",
      "49/85 [================>.............] - ETA: 0s - loss: 7.6763 - accuracy: 0.5023\n",
      "ROC-AUC Train: 0.532 - ROC-AUC Test: 0.5387                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.0 - Testing F1-Score: 0.0                                                                                                    \n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.50967\n",
      "85/85 [==============================] - 1s 13ms/step - loss: 7.7388 - accuracy: 0.4983 - val_loss: 7.5634 - val_accuracy: 0.5097\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x154026c8d00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_2\n",
    "\n",
    "optimizer_SGD = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.001, momentum=0.9, nesterov=False, name='SGD')\n",
    "\n",
    "filepath=\"model_autosaved/model_2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, \n",
    "                             monitor='val_accuracy',  verbose=1, \n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "decay_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy', factor=0.9, patience=2, verbose=1,\n",
    "            mode='auto', min_delta=0.001, cooldown=0, min_lr=0.001)\n",
    "\n",
    "earlystop_accuracy = EarlyStopping(monitor='val_accuracy', min_delta=0.00001, patience=2, verbose=1)\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/model_2\", histogram_freq=1)\n",
    "\n",
    "callback_list = [ROC_AUC, F1Score, checkpoint, decay_lr, \n",
    "                 lr_scheduler, terminate_NaN, earlystop_accuracy, \n",
    "                 tensorboard_callback ]\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer= optimizer_SGD, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), batch_size=200,  \n",
    "          callbacks= callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 205\n",
      "Trainable params: 205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(10, activation='relu', kernel_initializer= tf.keras.initializers.he_uniform(), input_shape=(n_features,)))\n",
    "model_3.add(Dense(8, activation='relu', kernel_initializer= tf.keras.initializers.he_uniform()))\n",
    "model_3.add(Dense(6, activation='relu', kernel_initializer= tf.keras.initializers.he_uniform()))\n",
    "model_3.add(Dense(4, activation='relu', kernel_initializer= tf.keras.initializers.he_uniform()))\n",
    "model_3.add(Dense(1))\n",
    "\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.009499999787658453.\n",
      "Epoch 1/100\n",
      " 2/85 [..............................] - ETA: 43s - loss: 3.9262 - accuracy: 0.4450WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 1.0337s). Check your callbacks.\n",
      "84/85 [============================>.] - ETA: 0s - loss: 0.7847 - accuracy: 0.4973\n",
      "ROC-AUC Train: 0.5009 - ROC-AUC Test: 0.5007                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.66818 - Testing F1-Score: 0.65802                                                                                                    \n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.49033, saving model to model_autosaved\\model_3.hdf5\n",
      "85/85 [==============================] - 3s 36ms/step - loss: 0.7837 - accuracy: 0.4971 - val_loss: 0.6932 - val_accuracy: 0.4903\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009499999694526196.\n",
      "Epoch 2/100\n",
      "81/85 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.5007\n",
      "ROC-AUC Train: 0.5028 - ROC-AUC Test: 0.5024                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.01141 - Testing F1-Score: 0.01215                                                                                                    \n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.49033 to 0.51233, saving model to model_autosaved\\model_3.hdf5\n",
      "85/85 [==============================] - 1s 16ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6927 - val_accuracy: 0.5123\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.009499999694526196.\n",
      "Epoch 3/100\n",
      "65/85 [=====================>........] - ETA: 0s - loss: 0.6914 - accuracy: 0.5112\n",
      "ROC-AUC Train: 0.6278 - ROC-AUC Test: 0.6065                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.63749 - Testing F1-Score: 0.61898                                                                                                    \n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.51233 to 0.57567, saving model to model_autosaved\\model_3.hdf5\n",
      "85/85 [==============================] - 1s 15ms/step - loss: 0.6883 - accuracy: 0.5262 - val_loss: 0.6912 - val_accuracy: 0.5757\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1547f72fdc0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_3\n",
    "\n",
    "optimizer_SGD = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.9, nesterov=True, name='SGD')\n",
    "\n",
    "filepath=\"model_autosaved/model_3.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, \n",
    "                             monitor='val_accuracy',  verbose=1, \n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "decay_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy', factor=0.9, patience=1, verbose=1,\n",
    "            mode='auto', min_delta=0.001, cooldown=0, min_lr=0.001)\n",
    "\n",
    "earlystop_accuracy = EarlyStopping(monitor='val_accuracy', min_delta=0.35, patience=2, verbose=1)\n",
    "\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/model_3\", histogram_freq=1)\n",
    "\n",
    "callback_list = [ROC_AUC, F1Score, checkpoint, decay_lr, \n",
    "                 lr_scheduler, terminate_NaN, earlystop_accuracy, \n",
    "                 tensorboard_callback ]\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer= optimizer_SGD, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test), batch_size=200,  \n",
    "          callbacks = callback_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch != 1 and epoch % 3 ==0:\n",
    "        return lr*0.95\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "lr_scheduler = LearningRateScheduler(scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 40)                120       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 55        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 5)                 20        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 634\n",
      "Trainable params: 624\n",
      "Non-trainable params: 10\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Dense(40, activation='relu', kernel_initializer= tf.keras.initializers.he_uniform(), input_shape=(n_features,)))\n",
    "model_4.add(Dense(10, activation='relu', kernel_initializer= tf.keras.initializers.he_uniform()))\n",
    "model_4.add(Dropout(0.2))\n",
    "model_4.add(Dense(5, activation='relu', kernel_initializer= tf.keras.initializers.he_uniform()))\n",
    "model_4.add(BatchNormalization())\n",
    "model_4.add(Dropout(0.5))\n",
    "model_4.add(Dense(4, activation='sigmoid', kernel_initializer= tf.keras.initializers.he_uniform()))\n",
    "model_4.add(Dense(1))\n",
    "\n",
    "print(model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_SGD = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.1, momentum=0.9, nesterov=True, \n",
    "    name='SGD')\n",
    "\n",
    "optimizer_aadam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True,\n",
    "    name='Adam')\n",
    "\n",
    "optimizer_adamax = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Adamax')\n",
    "\n",
    "optimizer_RMSprop = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.1, rho=0.9, momentum=0.8, epsilon=1e-07, centered=False,\n",
    "    name='RMSprop')\n",
    "\n",
    "optimizer_adagrad = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=0.1, initial_accumulator_value=0.1, epsilon=1e-07,\n",
    "    name='Adagrad')\n",
    "\n",
    "optimizer_adadelta = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.1, rho=0.95, epsilon=1e-07, name='Adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.09500000141561031.\n",
      "Epoch 1/50\n",
      " 2/85 [..............................] - ETA: 51s - loss: 1.2207 - accuracy: 0.5325WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_train_batch_end` time: 1.2446s). Check your callbacks.\n",
      "65/85 [=====================>........] - ETA: 0s - loss: 0.7162 - accuracy: 0.5267\n",
      "ROC-AUC Train: 0.6434 - ROC-AUC Test: 0.6443                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.30093 - Testing F1-Score: 0.29846                                                                                                    \n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.54400, saving model to model_autosaved\\model_4.hdf5\n",
      "85/85 [==============================] - 3s 39ms/step - loss: 0.7098 - accuracy: 0.5295 - val_loss: 0.6802 - val_accuracy: 0.5440\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0949999988079071.\n",
      "Epoch 2/50\n",
      "74/85 [=========================>....] - ETA: 0s - loss: 0.6846 - accuracy: 0.5460\n",
      "ROC-AUC Train: 0.6959 - ROC-AUC Test: 0.6961                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.59944 - Testing F1-Score: 0.60441                                                                                                    \n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.54400 to 0.64133, saving model to model_autosaved\\model_4.hdf5\n",
      "85/85 [==============================] - 1s 16ms/step - loss: 0.6835 - accuracy: 0.5480 - val_loss: 0.6681 - val_accuracy: 0.6413\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0949999988079071.\n",
      "Epoch 3/50\n",
      "85/85 [==============================] - ETA: 0s - loss: 0.6703 - accuracy: 0.5808\n",
      "ROC-AUC Train: 0.7299 - ROC-AUC Test: 0.73                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.60808 - Testing F1-Score: 0.61267                                                                                                    \n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.64133 to 0.66367, saving model to model_autosaved\\model_4.hdf5\n",
      "85/85 [==============================] - 1s 15ms/step - loss: 0.6703 - accuracy: 0.5808 - val_loss: 0.6486 - val_accuracy: 0.6637\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.09024999886751174.\n",
      "Epoch 4/50\n",
      "72/85 [========================>.....] - ETA: 0s - loss: 0.6636 - accuracy: 0.5900\n",
      "ROC-AUC Train: 0.7227 - ROC-AUC Test: 0.726                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.69255 - Testing F1-Score: 0.69314                                                                                                    \n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.66367\n",
      "85/85 [==============================] - 1s 15ms/step - loss: 0.6614 - accuracy: 0.5941 - val_loss: 0.6428 - val_accuracy: 0.6393\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.09025000035762787.\n",
      "Epoch 5/50\n",
      "71/85 [========================>.....] - ETA: 0s - loss: 0.6570 - accuracy: 0.6120\n",
      "ROC-AUC Train: 0.7333 - ROC-AUC Test: 0.7354                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.69753 - Testing F1-Score: 0.69781                                                                                                    \n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.66367\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.08122500032186508.\n",
      "85/85 [==============================] - 1s 14ms/step - loss: 0.6576 - accuracy: 0.6131 - val_loss: 0.6351 - val_accuracy: 0.6593\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.08122500032186508.\n",
      "Epoch 6/50\n",
      "63/85 [=====================>........] - ETA: 0s - loss: 0.6573 - accuracy: 0.6190\n",
      "ROC-AUC Train: 0.7316 - ROC-AUC Test: 0.7325                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.66959 - Testing F1-Score: 0.65914                                                                                                    \n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.66367\n",
      "85/85 [==============================] - 1s 16ms/step - loss: 0.6569 - accuracy: 0.6172 - val_loss: 0.6319 - val_accuracy: 0.6577\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.07716375030577183.\n",
      "Epoch 7/50\n",
      "64/85 [=====================>........] - ETA: 0s - loss: 0.6489 - accuracy: 0.6258\n",
      "ROC-AUC Train: 0.7339 - ROC-AUC Test: 0.7375                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.69262 - Testing F1-Score: 0.69707                                                                                                    \n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.66367 to 0.66567, saving model to model_autosaved\\model_4.hdf5\n",
      "85/85 [==============================] - 1s 14ms/step - loss: 0.6468 - accuracy: 0.6298 - val_loss: 0.6202 - val_accuracy: 0.6657\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.07716374844312668.\n",
      "Epoch 8/50\n",
      "68/85 [=======================>......] - ETA: 0s - loss: 0.6508 - accuracy: 0.6354\n",
      "ROC-AUC Train: 0.7331 - ROC-AUC Test: 0.7327                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.69777 - Testing F1-Score: 0.68945                                                                                                    \n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.66567\n",
      "85/85 [==============================] - 1s 14ms/step - loss: 0.6501 - accuracy: 0.6352 - val_loss: 0.6257 - val_accuracy: 0.6517\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.07716374844312668.\n",
      "Epoch 9/50\n",
      "71/85 [========================>.....] - ETA: 0s - loss: 0.6388 - accuracy: 0.6439\n",
      "ROC-AUC Train: 0.7235 - ROC-AUC Test: 0.7264                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.64263 - Testing F1-Score: 0.64708                                                                                                    \n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.66567\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.06944737359881402.\n",
      "85/85 [==============================] - 1s 17ms/step - loss: 0.6415 - accuracy: 0.6378 - val_loss: 0.6197 - val_accuracy: 0.6597\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.06597500704228877.\n",
      "Epoch 10/50\n",
      "64/85 [=====================>........] - ETA: 0s - loss: 0.6438 - accuracy: 0.6332\n",
      "ROC-AUC Train: 0.7372 - ROC-AUC Test: 0.7393                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.70297 - Testing F1-Score: 0.69968                                                                                                    \n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.66567\n",
      "85/85 [==============================] - 1s 14ms/step - loss: 0.6473 - accuracy: 0.6319 - val_loss: 0.6241 - val_accuracy: 0.6543\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.06597501039505005.\n",
      "Epoch 11/50\n",
      "73/85 [========================>.....] - ETA: 0s - loss: 0.6457 - accuracy: 0.6343\n",
      "ROC-AUC Train: 0.7296 - ROC-AUC Test: 0.7287                                                                                                    \n",
      "\n",
      "Training F1-Score: 0.67555 - Testing F1-Score: 0.67722                                                                                                    \n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.66567 to 0.66700, saving model to model_autosaved\\model_4.hdf5\n",
      "85/85 [==============================] - 1s 15ms/step - loss: 0.6454 - accuracy: 0.6342 - val_loss: 0.6223 - val_accuracy: 0.6670\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.06597501039505005.\n",
      "Epoch 12/50\n",
      "63/85 [=====================>........] - ETA: 0s - loss: 0.6523 - accuracy: 0.6261\n",
      "ROC-AUC Train: 0.7342 - ROC-AUC Test: 0.734                                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training F1-Score: 0.6954 - Testing F1-Score: 0.69656                                                                                                    \n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.66700\n",
      "85/85 [==============================] - 1s 15ms/step - loss: 0.6529 - accuracy: 0.6232 - val_loss: 0.6286 - val_accuracy: 0.6617\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1547f99ea30>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_4\n",
    "\n",
    "filepath=\"model_autosaved/model_4.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, save_weights_only=True, \n",
    "                             monitor='val_accuracy',  verbose=1, \n",
    "                             save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "decay_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy', factor=0.9, patience=2, verbose=1,\n",
    "            mode='auto', min_delta=0.001, cooldown=0, min_lr=0)\n",
    "\n",
    "earlystop_accuracy = EarlyStopping(monitor='val_accuracy', min_delta=0.035, patience=10, verbose=1)\n",
    "\n",
    "#log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs/model_4\", histogram_freq=1)\n",
    "\n",
    "callback_list = [ROC_AUC, F1Score, checkpoint, decay_lr, \n",
    "                 lr_scheduler, terminate_NaN, earlystop_accuracy, \n",
    "                 tensorboard_callback ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer= optimizer_adagrad, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test), batch_size=200,  \n",
    "          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://github.com/tensorflow/tensorboard/issues/2481\n",
    "\n",
    "#from tensorflow.python.framework import ops\n",
    "#ops.reset_default_graph()\n",
    "\n",
    "\n",
    "#logs_base_dir = \"./logs\"\n",
    "#os.makedirs(logs_base_dir, exist_ok=True)\n",
    "#%tensorboard --logdir {logs_base_dir}  --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b0f51d8c43a3b8e0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b0f51d8c43a3b8e0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Call_Backs_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
